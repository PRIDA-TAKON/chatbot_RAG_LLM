# Chatbot RAG with LLM

This project implements a Retrieval-Augmented Generation (RAG) chatbot. It uses a language model to answer questions based on a provided knowledge base, which is built from the `agnos_forum_data.json` file.

The application is designed with a decoupled architecture, separating the user interface (Frontend) from the AI processing (Backend) for scalability and performance.

---

## üèõÔ∏è Architecture

The system consists of two main components:

1.  **Backend (FastAPI):** 
    - A Python application built with FastAPI.
    - Responsible for all heavy lifting: loading the embedding and LLM models, creating the FAISS vector store, and running the RAG inference pipeline.
    - Exposes a single API endpoint (`/query`) to receive questions and return AI-generated answers.
    - Designed to be containerized with Docker and deployed on a cloud service like Hugging Face Spaces.

2.  **Frontend (Streamlit):**
    - A lightweight Python application built with Streamlit.
    - Provides a simple, clean chat interface for the user.
    - Does **not** load any models or perform any AI processing.
    - When a user asks a question, it makes an HTTP request to the Backend API, receives the answer, and displays it.
    - Designed to be deployed on Streamlit Community Cloud.

### Flow Diagram

```
[User] <--> [Streamlit Frontend (UI)] <--> [FastAPI Backend (API on Hugging Face Spaces)] <--> [LLM & Vector Store]
```

---

## üöÄ Getting Started

### 1. Initial Setup

First, you need to prepare the knowledge base and install dependencies.

-   **Clone the repository:**
    ```bash
    git clone <repository_url>
    cd 04-chatbot_RAG_LLM
    ```

-   **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

-   **Create the Vector Store:**
    Make sure the `agnos_forum_data.json` file is in the root directory. Then, run the `rag_pipeline.py` script **once** to create the `faiss_index` directory.
    ```bash
    python rag_pipeline.py
    ```

### 2. Running Locally for Testing

To test the full application on your local machine, you need to run both the backend and frontend simultaneously in two separate terminals.

-   **Terminal 1: Start the Backend API**
    ```bash
    uvicorn backend_api:app --reload
    ```
    Wait for the message indicating that the application startup is complete. The API will be available at `http://localhost:8000`.

-   **Terminal 2: Start the Frontend UI**
    ```bash
    streamlit run chatbot_interface.py
    ```
    A new browser tab will open with the Streamlit chat interface. You can now ask questions.

---

## ‚òÅÔ∏è Deployment

Deploying this application to the cloud involves two main steps:

### Step 1: Deploy the Backend to Hugging Face Spaces

The backend is deployed as a Docker image on Hugging Face Spaces.

1.  **‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö FAISS Index:**
    *   ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πÄ‡∏£‡∏Å‡∏ó‡∏≠‡∏£‡∏µ `faiss_index` (‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå `index.faiss` ‡πÅ‡∏•‡∏∞ `index.pkl`) ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÑ‡∏î‡πÄ‡∏£‡∏Å‡∏ó‡∏≠‡∏£‡∏µ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
    *   ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á FAISS index ‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞ build Docker image

2.  **Build Docker Image:**
    *   ‡πÄ‡∏õ‡∏¥‡∏î Terminal ‡∏´‡∏£‡∏∑‡∏≠ Command Prompt ‡πÉ‡∏ô‡πÑ‡∏î‡πÄ‡∏£‡∏Å‡∏ó‡∏≠‡∏£‡∏µ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
    *   ‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Docker image:
        ```bash
        docker build -t your-huggingface-username/your-space-name .
        ```
        *   ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà `your-huggingface-username` ‡∏î‡πâ‡∏ß‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ Hugging Face ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
        *   ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà `your-space-name` ‡∏î‡πâ‡∏ß‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Space ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì (‡πÄ‡∏ä‡πà‡∏ô `my-llm-backend`)
        *   ‡∏≠‡∏¢‡πà‡∏≤‡∏•‡∏∑‡∏°‡∏à‡∏∏‡∏î `.` ‡∏ó‡∏µ‡πà‡∏ó‡πâ‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á ‡∏ã‡∏∂‡πà‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á Dockerfile ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÑ‡∏î‡πÄ‡∏£‡∏Å‡∏ó‡∏≠‡∏£‡∏µ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô

3.  **Log in ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà Hugging Face CLI:**
    *   ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á `huggingface_hub` ‡πÉ‡∏´‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏Å‡πà‡∏≠‡∏ô: `pip install huggingface_hub`
    *   ‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö Hugging Face:
        ```bash
        huggingface-cli login
        ```
    *   ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏Ç‡∏≠ Hugging Face Token ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ Settings -> Access Tokens ‡∏ö‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå Hugging Face

4.  **Push Docker Image ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Hugging Face Spaces:**
    *   ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å build image ‡πÅ‡∏•‡∏∞ login ‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠ push image ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Hugging Face Container Registry:
        ```bash
        docker push your-huggingface-username/your-space-name
        ```
        *   ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ image ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2

5.  **‡∏™‡∏£‡πâ‡∏≤‡∏á Space ‡πÉ‡∏´‡∏°‡πà‡∏ö‡∏ô Hugging Face:**
    *   ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå Hugging Face (huggingface.co)
    *   ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏ó‡∏µ‡πà‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å "New Space"
    *   ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠ Space ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì (‡∏Ñ‡∏ß‡∏£‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö `your-space-name` ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Docker image)
    *   ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å "Docker" ‡πÄ‡∏õ‡πá‡∏ô SDK
    *   ‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô "Docker image", ‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠ Docker image ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì push ‡πÑ‡∏õ (‡πÄ‡∏ä‡πà‡∏ô `your-huggingface-username/your-space-name`)
    *   ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Æ‡∏≤‡∏£‡πå‡∏î‡πÅ‡∏ß‡∏£‡πå (Hardware) ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡∏´‡∏≤‡∏Å LLM ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ GPU ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å GPU ‡∏ó‡∏µ‡πà‡∏°‡∏µ
    *   ‡∏Ñ‡∏•‡∏¥‡∏Å "Create Space"

### Step 2: Deploy the Frontend to Streamlit Cloud

1.  **Push to GitHub:** Ensure your latest code, especially `chatbot_interface.py`, is pushed to your GitHub repository.
2.  **Deploy on Streamlit:** Connect your GitHub repository to Streamlit Cloud and deploy the `chatbot_interface.py` application.
3.  **Set the Secret:** In your Streamlit app's settings, go to **Settings > Secrets** and add a new secret:
    -   **Name:** `BACKEND_API_URL`
    -   **Value:** Paste the public URL of your Hugging Face Space (e.g., `https://your-huggingface-username-your-space-name.hf.space`).

Once the secret is saved, the Streamlit app will automatically restart and will now send requests to your high-performance backend on Hugging Face Spaces, resulting in a much faster and more stable experience.